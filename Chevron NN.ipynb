{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yalex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\yalex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\yalex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\yalex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\yalex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\yalex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_id               6838\n",
      "wellbore_chev_no_id        21\n",
      "area_id                     2\n",
      "formation_id              239\n",
      "bit_model_id               38\n",
      "drillbit_size               7\n",
      "min_depth                 990\n",
      "max_depth                1542\n",
      "rate_of_penetration      6838\n",
      "surface_weight_on_bit    6837\n",
      "surface_rpm              6210\n",
      "dtype: int64\n",
      "segment_id               1567\n",
      "wellbore_chev_no_id         4\n",
      "area_id                     2\n",
      "formation_id              118\n",
      "bit_model_id               11\n",
      "drillbit_size               6\n",
      "min_depth                 836\n",
      "max_depth                 840\n",
      "surface_weight_on_bit    1567\n",
      "surface_rpm              1465\n",
      "dtype: int64\n",
      "(6838, 10)\n",
      "WARNING:tensorflow:From C:\\Users\\yalex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 6838 into shape (1567,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c6e781fd7a09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[0my_pred5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m \u001b[0my_pred5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1567\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;31m#rmse_gb = math.sqrt(mean_squared_error(y_test, y_pred5))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;31m#print(\"RMSE: %.4f\" % rmse_gb)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    299\u001b[0m            [5, 6]])\n\u001b[0;32m    300\u001b[0m     \"\"\"\n\u001b[1;32m--> 301\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 6838 into shape (1567,1)"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "#input csv file\n",
    "training_data = pd.read_csv('training.csv', delimiter = ',') #training data frame\n",
    "\n",
    "testing_data = pd.read_csv('scoring.csv', delimiter = ',') #testing data frame\n",
    "\n",
    "\n",
    "y = np.array(training_data['rate_of_penetration'])\n",
    "\n",
    "def parse_data(data):\n",
    "    #parse out annoying strings in front \n",
    "    #training_data['segment_id'] =  training_data['segment_id'].apply(lambda x: int(x[10:]))\n",
    "    data['wellbore_chev_no_id'] =  data['wellbore_chev_no_id'].apply(lambda x: int(x[17:]))\n",
    "    data['area_id'] =  data['area_id'].apply(lambda x: int(x[5:]))\n",
    "    data['formation_id'] =  data['formation_id'].apply(lambda x: int(x[10:]))\n",
    "    data['bit_model_id'] =  data['bit_model_id'].apply(lambda x: int(x[10:]))\n",
    "    return data\n",
    "    \n",
    "\n",
    "print (training_data.nunique())\n",
    "print (testing_data.nunique())\n",
    "\n",
    "training_data = parse_data(training_data.drop(['rate_of_penetration'], axis = 1))\n",
    "testing_data = parse_data(testing_data)\n",
    "\n",
    "print (training_data.shape)\n",
    "\n",
    "training_length = training_data.shape[0] #how many training examples you have\n",
    "\n",
    "concat = pd.concat([training_data, testing_data]) #concat the two to do one hot encoding\n",
    "\n",
    "\n",
    "#one-hot encoding\n",
    "def one_hot(data1):\n",
    "    X = pd.get_dummies(data=data1, columns=['wellbore_chev_no_id', 'area_id', 'formation_id', 'bit_model_id', 'drillbit_size'])\n",
    "    cols_to_norm = ['min_depth','max_depth', 'surface_weight_on_bit', 'surface_rpm']\n",
    "    X[cols_to_norm] = X[cols_to_norm].apply(lambda x: preprocessing.scale(x)) #only standardize non-one-hot columns\n",
    "    #print (X)\n",
    "    X = np.array(X.drop(['segment_id','max_depth'], axis=1))\n",
    "    return X\n",
    "\n",
    "#Models\n",
    "\n",
    "overall_one_hot = np.array(one_hot(concat)) #one-hot on the training + testing sets combined to take care of all ids\n",
    "\n",
    "training_data = overall_one_hot[:training_length]\n",
    "testing_data = overall_one_hot[training_length:]\n",
    "\n",
    "#print (training_data)\n",
    "\n",
    "#print (training_data.isnull().sum(axis = 0)) #no null values \n",
    "\n",
    "\n",
    "\n",
    "#y = preprocessing.scale(y)\n",
    "X = training_data #define training matrix of examples \n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import backend\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#grid search for parameters\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import optimizers #try adam and sgd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "    \n",
    "    \n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "\n",
    "#split training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0, random_state=42) #already have final test set, so train\n",
    "\n",
    "X_test = testing_data #testing matrix of examples\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(2, input_dim=324, activation='relu'))\n",
    "model1.add(Dense(5, kernel_initializer='normal',activation='relu'))\n",
    "model1.add(Dense(5, kernel_initializer='normal',activation='relu'))\n",
    "model1.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "\n",
    "#IMPORTANT - use this if you want to train, and comment out load weights\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model1.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "#history = model1.fit(X_train, y_train, validation_split=0.05, epochs=500, batch_size=100, verbose=2)\n",
    "#model1.save_weights(\"NN.h5\")\n",
    "\n",
    "\n",
    "#IMPORTANT - load pre-existing weights for best model, no need to train\n",
    "model1.load_weights('NN.h5')\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(2, input_dim=324, activation='relu'))\n",
    "model2.add(Dense(7, kernel_initializer='normal',activation='relu'))\n",
    "model2.add(Dense(7, kernel_initializer='normal',activation='relu'))\n",
    "model2.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "model2.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "#history = model2.fit(X_train, y_train, validation_split=0.05, epochs=500, batch_size=150, verbose=2)\n",
    "#model2.save_weights(\"NN2.h5\")\n",
    "\n",
    "model2.load_weights('NN2.h5')\n",
    "\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(2, input_dim=324, activation='relu'))\n",
    "model3.add(Dense(6, kernel_initializer='normal',activation='relu'))\n",
    "model3.add(Dense(6, kernel_initializer='normal',activation='relu'))\n",
    "model3.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "model3.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "#history = model3.fit(X_train, y_train, validation_split=0.05, epochs=500, batch_size=150, verbose=2)\n",
    "#model3.save_weights(\"NN3.h5\")\n",
    "\n",
    "model3.load_weights('NN3.h5')\n",
    "\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(2, input_dim=324, activation='relu'))\n",
    "model4.add(Dense(8, kernel_initializer='normal',activation='relu'))\n",
    "model4.add(Dense(8, kernel_initializer='normal',activation='relu'))\n",
    "model4.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "model4.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "#history = model4.fit(X_train, y_train, validation_split=0.05, epochs=500, batch_size=150, verbose=2)\n",
    "#model4.save_weights(\"NN4.h5\")\n",
    "\n",
    "#try different activation functions\n",
    "#different optimizers \n",
    "\n",
    "model4.load_weights('NN4.h5')\n",
    "\n",
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "params = {'n_estimators': 500, 'max_depth': 5, 'min_samples_split': 3, #3 samples fit, 250/10 vs 500/5\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "filename = 'GBoost1_m.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "params2 = {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 3, #3 samples fit, 250/10 vs 500/5\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf2 = ensemble.GradientBoostingRegressor(**params2)\n",
    "clf2.fit(X_train, y_train)\n",
    "filename = 'GBoost2_m.sav'\n",
    "pickle.dump(clf2, open(filename, 'wb'))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "gb1 = pickle.load(open('Gboost1_m.sav', 'rb'))\n",
    "gb2 = pickle.load(open('Gboost2_m.sav', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred5 = gb1.predict(X)\n",
    "y_pred5 = np.reshape(y_pred5,(1567,1))\n",
    "#rmse_gb = math.sqrt(mean_squared_error(y_test, y_pred5))\n",
    "#print(\"RMSE: %.4f\" % rmse_gb)\n",
    "\n",
    "\n",
    "y_pred6 = gb2.predict(X)\n",
    "y_pred6 = np.reshape(y_pred6,(1567,1))\n",
    "#rmse_gb = math.sqrt(mean_squared_error(y_test, y_pred6))\n",
    "#print(\"RMSE: %.4f\" % rmse_gb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "use this to calc rmse between two vectors of the same length\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def rmse_non_tensor(a,b):\n",
    "    total = 0\n",
    "    for i in range(len(a)):\n",
    "        total += (a[i]-b[i])**2\n",
    "    total = total/len(a)\n",
    "    total = math.sqrt(total)\n",
    "    return total\n",
    "\n",
    "y_pred1 = model1.predict(X) #model prediction on X test set\n",
    "y_pred2 = model2.predict(X) #model prediction on X test set\n",
    "y_pred3 = model3.predict(X) #model prediction on X test set\n",
    "y_pred4 = model4.predict(X) #model prediction on X test set\n",
    "\n",
    "\n",
    "#Ensemble of 4 ANNs + Gradient Boost Decision Tree Emsemble Regressor\n",
    "y_pred = ((0.1*(y_pred1 + y_pred2 + y_pred3 + y_pred4 + y_pred5) + 0.5*y_pred6))\n",
    "\n",
    "#y_pred = ((y_pred5 + y_pred6)/2.0)\n",
    "#y_test = np.reshape(y_test,(1567,1)) #no y_test\n",
    "#print (y_pred.shape)\n",
    "#print (y_test.shape)\n",
    "\n",
    "#print(math.sqrt(mean_squared_error(y_pred, y)))\n",
    "testing_labels = np.array(pd.read_csv('scoring.csv', delimiter = ',')['segment_id']) #testing data frame\n",
    "testing_labels = np.reshape(testing_labels, (1567,1))\n",
    "final = np.concatenate((testing_labels, y_pred), axis=1) #test is actual, pred is prediction\n",
    "final = pd.DataFrame(final)\n",
    "final.columns = ['segment_id', 'rate_of_penetration']\n",
    "#export final prediction csv\n",
    "export_csv = final.to_csv ('submission.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#summarize history for rmse\n",
    "plt.plot(history.history['rmse'])\n",
    "plt.plot(history.history['val_rmse'])\n",
    "plt.title('Model RMSE')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#cross val\n",
    "\n",
    "from sklearn.model_selection import KFold #K fold cross validation \n",
    "#X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "#y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "   \n",
    "   #lsvc = linear_model.Lasso(alpha=1, max_iter = 1000).fit(X_train, y_train)\n",
    "    #y_pred = lsvc.predict(X_test)\n",
    "    #regressor = RandomForestRegressor(n_estimators=100, max_depth = 15, random_state=0)\n",
    "    #regressor.fit(X_train, y_train)\n",
    "    #y_pred = regressor.predict(X_test)\n",
    "    #print (y_pred)\n",
    "    #print (y_test)\n",
    "    #print (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(2, input_dim=311, activation='linear'))\n",
    "    model.add(Dense(5, kernel_initializer='normal',activation='linear'))\n",
    "    model.add(Dense(5, kernel_initializer='normal',activation='linear'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    history = model.fit(X_train, y_train, epochs=500, batch_size=100, verbose=2)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_id               6838\n",
      "wellbore_chev_no_id        21\n",
      "area_id                     2\n",
      "formation_id              239\n",
      "bit_model_id               38\n",
      "drillbit_size               7\n",
      "min_depth                 990\n",
      "max_depth                1542\n",
      "rate_of_penetration      6838\n",
      "surface_weight_on_bit    6837\n",
      "surface_rpm              6210\n",
      "dtype: int64\n",
      "RMSE: 21.3100\n",
      "RMSE: 18.8120\n",
      "19.592554397507538\n",
      "          Actual  Prediction\n",
      "0     227.103109  247.493798\n",
      "1      78.116084   67.972360\n",
      "2      55.749590   63.659276\n",
      "3      46.947824   56.502388\n",
      "4     163.160333  165.257118\n",
      "5     105.309330  105.530433\n",
      "6     115.418377  123.424452\n",
      "7     138.143219  136.061614\n",
      "8     102.287113  105.429853\n",
      "9      77.027467   59.092847\n",
      "10    120.285998  110.343820\n",
      "11     78.117781   80.421242\n",
      "12     71.308891   84.721874\n",
      "13     58.593417   68.198066\n",
      "14     43.936250   73.646898\n",
      "15    191.948166  182.265445\n",
      "16     99.284859   87.001610\n",
      "17    121.356377  117.738209\n",
      "18     89.865083   94.361298\n",
      "19    131.967490  122.050480\n",
      "20     96.276885   52.132777\n",
      "21     40.215789   48.513214\n",
      "22    132.513884  140.214020\n",
      "23    140.771100  140.105183\n",
      "24    146.576500  166.008234\n",
      "25    142.529131  137.164082\n",
      "26    103.356471  101.100128\n",
      "27     92.128548   95.992615\n",
      "28    146.182468  117.477147\n",
      "29     46.530410   57.985601\n",
      "...          ...         ...\n",
      "996    58.055424   57.620838\n",
      "997   146.542890  138.494996\n",
      "998    89.512805   90.168512\n",
      "999   200.064583  189.755787\n",
      "1000   52.527166   54.698114\n",
      "1001   78.217482   85.138020\n",
      "1002   90.912801   93.328824\n",
      "1003  221.863744  121.643115\n",
      "1004  167.389321  176.497640\n",
      "1005   73.675429   86.611841\n",
      "1006   45.025821   44.603260\n",
      "1007  145.647749  139.685929\n",
      "1008  103.314991  103.330354\n",
      "1009  115.950354  122.622733\n",
      "1010  173.292348  149.095289\n",
      "1011  100.799642   96.339438\n",
      "1012   97.679999  126.453777\n",
      "1013   35.703866   43.402342\n",
      "1014  121.069904  111.805283\n",
      "1015   31.829818   57.621825\n",
      "1016   60.922934   72.844881\n",
      "1017  138.614075  140.571752\n",
      "1018   82.921833   98.983848\n",
      "1019  151.625250  133.542381\n",
      "1020   52.440962   59.075299\n",
      "1021  144.875417   95.102146\n",
      "1022  150.405495  146.456532\n",
      "1023   71.754801   78.563664\n",
      "1024  355.846390  318.218220\n",
      "1025   61.407978   60.002519\n",
      "\n",
      "[1026 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#summarize history for rmse\\nplt.plot(history.history[\\'rmse\\'])\\nplt.plot(history.history[\\'val_rmse\\'])\\nplt.title(\\'Model RMSE\\')\\nplt.ylabel(\\'RMSE\\')\\nplt.xlabel(\\'Epoch\\')\\nplt.legend([\\'train\\', \\'validation\\'], loc=\\'upper left\\')\\nplt.show()\\n\\n# summarize history for loss\\nplt.plot(history.history[\\'loss\\'])\\nplt.plot(history.history[\\'val_loss\\'])\\nplt.title(\\'Model Loss\\')\\nplt.ylabel(\\'Loss\\')\\nplt.xlabel(\\'Epoch\\')\\nplt.legend([\\'train\\', \\'validation\\'], loc=\\'upper left\\')\\nplt.show()\\n\\n\\n#cross val\\n\\nfrom sklearn.model_selection import KFold #K fold cross validation \\n#X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\\n#y = np.array([1, 2, 3, 4])\\nkf = KFold(n_splits=10)\\nkf.get_n_splits(X)\\n\\nfor train_index, test_index in kf.split(X):\\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\\n    X_train, X_test = X[train_index], X[test_index]\\n    y_train, y_test = y[train_index], y[test_index]\\n   \\n   #lsvc = linear_model.Lasso(alpha=1, max_iter = 1000).fit(X_train, y_train)\\n    #y_pred = lsvc.predict(X_test)\\n    #regressor = RandomForestRegressor(n_estimators=100, max_depth = 15, random_state=0)\\n    #regressor.fit(X_train, y_train)\\n    #y_pred = regressor.predict(X_test)\\n    #print (y_pred)\\n    #print (y_test)\\n    #print (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\\n    \\n    \\n    model = Sequential()\\n    model.add(Dense(2, input_dim=311, activation=\\'linear\\'))\\n    model.add(Dense(5, kernel_initializer=\\'normal\\',activation=\\'linear\\'))\\n    model.add(Dense(5, kernel_initializer=\\'normal\\',activation=\\'linear\\'))\\n    model.add(Dense(1))\\n    model.compile(loss=\\'mse\\', optimizer=\\'adam\\', metrics=[rmse])\\n    history = model.fit(X_train, y_train, epochs=500, batch_size=100, verbose=2)\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "#input csv file\n",
    "training_data = pd.read_csv('training.csv', delimiter = ',') #training data frame\n",
    "\n",
    "\n",
    "#parse out annoying strings in front \n",
    "#training_data['segment_id'] =  training_data['segment_id'].apply(lambda x: int(x[10:]))\n",
    "training_data['wellbore_chev_no_id'] =  training_data['wellbore_chev_no_id'].apply(lambda x: int(x[17:]))\n",
    "training_data['area_id'] =  training_data['area_id'].apply(lambda x: int(x[5:]))\n",
    "training_data['formation_id'] =  training_data['formation_id'].apply(lambda x: int(x[10:]))\n",
    "training_data['bit_model_id'] =  training_data['bit_model_id'].apply(lambda x: int(x[10:]))\n",
    "\n",
    "#training_data.to_csv('Clean Data.csv')\n",
    "\n",
    "print (training_data.nunique())\n",
    "\n",
    "#print (training_data)\n",
    "\n",
    "#print (training_data.isnull().sum(axis = 0)) #no null values \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "y = np.array(training_data['rate_of_penetration'])\n",
    "#y = preprocessing.scale(y)\n",
    "X = np.array(training_data.drop(['segment_id','rate_of_penetration'], axis=1))\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import backend\n",
    " \n",
    "\n",
    "#one-hot encoding\n",
    "X = pd.get_dummies(data=training_data, columns=['wellbore_chev_no_id', 'area_id', 'formation_id', 'bit_model_id', 'drillbit_size'])\n",
    "cols_to_norm = ['min_depth','max_depth', 'surface_weight_on_bit', 'surface_rpm']\n",
    "X[cols_to_norm] = X[cols_to_norm].apply(lambda x: preprocessing.scale(x)) #only standardize non-one-hot columns\n",
    "#print (X)\n",
    "X = np.array(X.drop(['segment_id','max_depth', 'rate_of_penetration'], axis=1))\n",
    "#print(pd.get_dummies(training_data['wellbore_chev_no_id']))\n",
    "#print (X.nunique())\n",
    "\n",
    "\n",
    "#Models\n",
    "\n",
    "#grid search for parameters\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import optimizers #try adam and sgd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "#for a in [0.01, 0.05, 0.1, 0.5, 1, 5]:\n",
    "    #for m in [100,500,1000,5000,10000]:\n",
    "    \n",
    "    \n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "\n",
    "#split training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42) #split data (85% train, 15% test)\n",
    "\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(2, input_dim=310, activation='relu'))\n",
    "model1.add(Dense(5, kernel_initializer='normal',activation='relu'))\n",
    "model1.add(Dense(5, kernel_initializer='normal',activation='relu'))\n",
    "model1.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "\n",
    "#IMPORTANT - use this if you want to train, and comment out load weights\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model1.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "#history = model1.fit(X_train, y_train, validation_split=0.15, epochs=500, batch_size=100, verbose=2)\n",
    "#model1.save_weights(\"NN.h5\")\n",
    "\n",
    "\n",
    "#IMPORTANT - load pre-existing weights for best model, no need to train\n",
    "model1.load_weights('NN.h5')\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(2, input_dim=310, activation='relu'))\n",
    "model2.add(Dense(7, kernel_initializer='normal',activation='relu'))\n",
    "model2.add(Dense(7, kernel_initializer='normal',activation='relu'))\n",
    "model2.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "model2.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "#history = model2.fit(X_train, y_train, validation_split=0.15, epochs=500, batch_size=150, verbose=2)\n",
    "#model2.save_weights(\"NN2.h5\")\n",
    "\n",
    "model2.load_weights('NN2.h5')\n",
    "\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(2, input_dim=310, activation='relu'))\n",
    "model3.add(Dense(6, kernel_initializer='normal',activation='relu'))\n",
    "model3.add(Dense(6, kernel_initializer='normal',activation='relu'))\n",
    "model3.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "model3.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "#history = model3.fit(X_train, y_train, validation_split=0.15, epochs=500, batch_size=150, verbose=2)\n",
    "#model3.save_weights(\"NN3.h5\")\n",
    "\n",
    "model3.load_weights('NN3.h5')\n",
    "\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(2, input_dim=310, activation='relu'))\n",
    "model4.add(Dense(8, kernel_initializer='normal',activation='relu'))\n",
    "model4.add(Dense(8, kernel_initializer='normal',activation='relu'))\n",
    "model4.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "model4.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "#history = model4.fit(X_train, y_train, validation_split=0.15, epochs=500, batch_size=150, verbose=2)\n",
    "#model4.save_weights(\"NN4.h5\")\n",
    "\n",
    "#try different activation functions\n",
    "#different optimizers \n",
    "\n",
    "model4.load_weights('NN4.h5')\n",
    "\n",
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "params = {'n_estimators': 500, 'max_depth': 5, 'min_samples_split': 3, #3 samples fit, 250/10 vs 500/5\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "filename = 'GBoost1_m.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "params2 = {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 3, #3 samples fit, 250/10 vs 500/5\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf2 = ensemble.GradientBoostingRegressor(**params2)\n",
    "clf2.fit(X_train, y_train)\n",
    "filename = 'GBoost2_m.sav'\n",
    "pickle.dump(clf2, open(filename, 'wb'))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "gb1 = pickle.load(open('Gboost1.sav', 'rb'))\n",
    "gb2 = pickle.load(open('Gboost2.sav', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "y_pred5 = gb1.predict(X_test)\n",
    "y_pred5 = np.reshape(y_pred5,(1026,1))\n",
    "rmse_gb = math.sqrt(mean_squared_error(y_test, y_pred5))\n",
    "print(\"RMSE: %.4f\" % rmse_gb)\n",
    "\n",
    "\n",
    "y_pred6 = gb2.predict(X_test)\n",
    "y_pred6 = np.reshape(y_pred6,(1026,1))\n",
    "rmse_gb = math.sqrt(mean_squared_error(y_test, y_pred6))\n",
    "print(\"RMSE: %.4f\" % rmse_gb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "use this to calc rmse between two vectors of the same length\n",
    "\"\"\"\n",
    "\n",
    "def rmse_non_tensor(a,b):\n",
    "    total = 0\n",
    "    for i in range(len(a)):\n",
    "        total += (a[i]-b[i])**2\n",
    "    total = total/len(a)\n",
    "    total = math.sqrt(total)\n",
    "    return total\n",
    "\n",
    "y_pred1 = model1.predict(X_test) #model prediction on X test set\n",
    "y_pred2 = model2.predict(X_test) #model prediction on X test set\n",
    "y_pred3 = model3.predict(X_test) #model prediction on X test set\n",
    "y_pred4 = model4.predict(X_test) #model prediction on X test set\n",
    "\n",
    "\n",
    "#Ensemble of 4 ANNs + Gradient Boost Decision Tree Emsemble Regressor\n",
    "y_pred = ((0.1*(y_pred1 + y_pred2 + y_pred3 + y_pred4 + y_pred5) + 0.5*y_pred6))\n",
    "\n",
    "#y_pred = ((y_pred5 + y_pred6)/2.0)\n",
    "y_test = np.reshape(y_test,(1026,1))\n",
    "#print (y_pred.shape)\n",
    "#print (y_test.shape)\n",
    "\n",
    "print (rmse_non_tensor(y_pred,y_test))\n",
    "\n",
    "\n",
    "final = np.concatenate((y_test, y_pred), axis=1) #test is actual, pred is prediction\n",
    "\n",
    "final = pd.DataFrame(final)\n",
    "final.columns = ['Actual', 'Prediction']\n",
    "\n",
    "print(final)\n",
    "final = final.drop(['Actual'], axis = 1)\n",
    "export_csv = final.to_csv ('Prediction.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#summarize history for rmse\n",
    "plt.plot(history.history['rmse'])\n",
    "plt.plot(history.history['val_rmse'])\n",
    "plt.title('Model RMSE')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#cross val\n",
    "\n",
    "from sklearn.model_selection import KFold #K fold cross validation \n",
    "#X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "#y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "   \n",
    "   #lsvc = linear_model.Lasso(alpha=1, max_iter = 1000).fit(X_train, y_train)\n",
    "    #y_pred = lsvc.predict(X_test)\n",
    "    #regressor = RandomForestRegressor(n_estimators=100, max_depth = 15, random_state=0)\n",
    "    #regressor.fit(X_train, y_train)\n",
    "    #y_pred = regressor.predict(X_test)\n",
    "    #print (y_pred)\n",
    "    #print (y_test)\n",
    "    #print (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(2, input_dim=311, activation='linear'))\n",
    "    model.add(Dense(5, kernel_initializer='normal',activation='linear'))\n",
    "    model.add(Dense(5, kernel_initializer='normal',activation='linear'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    history = model.fit(X_train, y_train, epochs=500, batch_size=100, verbose=2)\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

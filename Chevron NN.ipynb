{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_id               6838\n",
      "wellbore_chev_no_id        21\n",
      "area_id                     2\n",
      "formation_id              239\n",
      "bit_model_id               38\n",
      "drillbit_size               7\n",
      "min_depth                 990\n",
      "max_depth                1542\n",
      "rate_of_penetration      6838\n",
      "surface_weight_on_bit    6837\n",
      "surface_rpm              6210\n",
      "dtype: int64\n",
      "segment_id               1567\n",
      "wellbore_chev_no_id         4\n",
      "area_id                     2\n",
      "formation_id              118\n",
      "bit_model_id               11\n",
      "drillbit_size               6\n",
      "min_depth                 836\n",
      "max_depth                 840\n",
      "surface_weight_on_bit    1567\n",
      "surface_rpm              1465\n",
      "dtype: int64\n",
      "(6838, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#summarize history for rmse\\nplt.plot(history.history[\\'rmse\\'])\\nplt.plot(history.history[\\'val_rmse\\'])\\nplt.title(\\'Model RMSE\\')\\nplt.ylabel(\\'RMSE\\')\\nplt.xlabel(\\'Epoch\\')\\nplt.legend([\\'train\\', \\'validation\\'], loc=\\'upper left\\')\\nplt.show()\\n\\n# summarize history for loss\\nplt.plot(history.history[\\'loss\\'])\\nplt.plot(history.history[\\'val_loss\\'])\\nplt.title(\\'Model Loss\\')\\nplt.ylabel(\\'Loss\\')\\nplt.xlabel(\\'Epoch\\')\\nplt.legend([\\'train\\', \\'validation\\'], loc=\\'upper left\\')\\nplt.show()\\n\\n\\n#cross val\\n\\nfrom sklearn.model_selection import KFold #K fold cross validation \\n#X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\\n#y = np.array([1, 2, 3, 4])\\nkf = KFold(n_splits=10)\\nkf.get_n_splits(X)\\n\\nfor train_index, test_index in kf.split(X):\\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\\n    X_train, X_test = X[train_index], X[test_index]\\n    y_train, y_test = y[train_index], y[test_index]\\n   \\n   #lsvc = linear_model.Lasso(alpha=1, max_iter = 1000).fit(X_train, y_train)\\n    #y_pred = lsvc.predict(X_test)\\n    #regressor = RandomForestRegressor(n_estimators=100, max_depth = 15, random_state=0)\\n    #regressor.fit(X_train, y_train)\\n    #y_pred = regressor.predict(X_test)\\n    #print (y_pred)\\n    #print (y_test)\\n    #print (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\\n    \\n    \\n    model = Sequential()\\n    model.add(Dense(2, input_dim=311, activation=\\'linear\\'))\\n    model.add(Dense(5, kernel_initializer=\\'normal\\',activation=\\'linear\\'))\\n    model.add(Dense(5, kernel_initializer=\\'normal\\',activation=\\'linear\\'))\\n    model.add(Dense(1))\\n    model.compile(loss=\\'mse\\', optimizer=\\'adam\\', metrics=[rmse])\\n    history = model.fit(X_train, y_train, epochs=500, batch_size=100, verbose=2)\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "#input csv file\n",
    "training_data = pd.read_csv('training.csv', delimiter = ',') #training data frame\n",
    "\n",
    "testing_data = pd.read_csv('scoring.csv', delimiter = ',') #testing data frame\n",
    "\n",
    "\n",
    "y = np.array(training_data['rate_of_penetration'])\n",
    "\n",
    "def parse_data(data):\n",
    "    #parse out annoying strings in front \n",
    "    #training_data['segment_id'] =  training_data['segment_id'].apply(lambda x: int(x[10:]))\n",
    "    data['wellbore_chev_no_id'] =  data['wellbore_chev_no_id'].apply(lambda x: int(x[17:]))\n",
    "    data['area_id'] =  data['area_id'].apply(lambda x: int(x[5:]))\n",
    "    data['formation_id'] =  data['formation_id'].apply(lambda x: int(x[10:]))\n",
    "    data['bit_model_id'] =  data['bit_model_id'].apply(lambda x: int(x[10:]))\n",
    "    return data\n",
    "    \n",
    "\n",
    "print (training_data.nunique())\n",
    "print (testing_data.nunique())\n",
    "\n",
    "training_data = parse_data(training_data.drop(['rate_of_penetration'], axis = 1))\n",
    "testing_data = parse_data(testing_data)\n",
    "\n",
    "print (training_data.shape)\n",
    "\n",
    "training_length = training_data.shape[0] #how many training examples you have\n",
    "\n",
    "concat = pd.concat([training_data, testing_data]) #concat the two to do one hot encoding\n",
    "\n",
    "\n",
    "#one-hot encoding\n",
    "def one_hot(data1):\n",
    "    X = pd.get_dummies(data=data1, columns=['wellbore_chev_no_id', 'area_id', 'formation_id', 'bit_model_id', 'drillbit_size'])\n",
    "    cols_to_norm = ['min_depth','max_depth', 'surface_weight_on_bit', 'surface_rpm']\n",
    "    X[cols_to_norm] = X[cols_to_norm].apply(lambda x: preprocessing.scale(x)) #only standardize non-one-hot columns\n",
    "    #print (X)\n",
    "    X = np.array(X.drop(['segment_id','max_depth'], axis=1))\n",
    "    return X\n",
    "\n",
    "#Models\n",
    "\n",
    "overall_one_hot = np.array(one_hot(concat)) #one-hot on the training + testing sets combined to take care of all ids\n",
    "\n",
    "training_data = overall_one_hot[:training_length]\n",
    "testing_data = overall_one_hot[training_length:]\n",
    "\n",
    "#print (training_data)\n",
    "\n",
    "#print (training_data.isnull().sum(axis = 0)) #no null values \n",
    "\n",
    "\n",
    "\n",
    "#y = preprocessing.scale(y)\n",
    "X = training_data #define training matrix of examples \n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import backend\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#grid search for parameters\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import optimizers #try adam and sgd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "    \n",
    "    \n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "\n",
    "#split training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0, random_state=42) #already have final test set, so train\n",
    "\n",
    "X_test = testing_data #testing matrix of examples\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(2, input_dim=324, activation='relu'))\n",
    "model1.add(Dense(5, kernel_initializer='normal',activation='relu'))\n",
    "model1.add(Dense(5, kernel_initializer='normal',activation='relu'))\n",
    "model1.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "\n",
    "#IMPORTANT - use this if you want to train, and comment out load weights\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model1.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "#history = model1.fit(X_train, y_train, validation_split=0.05, epochs=500, batch_size=100, verbose=2)\n",
    "#model1.save_weights(\"NN.h5\")\n",
    "\n",
    "\n",
    "#IMPORTANT - load pre-existing weights for best model, no need to train\n",
    "model1.load_weights('NN.h5')\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(2, input_dim=324, activation='relu'))\n",
    "model2.add(Dense(7, kernel_initializer='normal',activation='relu'))\n",
    "model2.add(Dense(7, kernel_initializer='normal',activation='relu'))\n",
    "model2.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "model2.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "#history = model2.fit(X_train, y_train, validation_split=0.05, epochs=500, batch_size=150, verbose=2)\n",
    "#model2.save_weights(\"NN2.h5\")\n",
    "\n",
    "model2.load_weights('NN2.h5')\n",
    "\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(2, input_dim=324, activation='relu'))\n",
    "model3.add(Dense(6, kernel_initializer='normal',activation='relu'))\n",
    "model3.add(Dense(6, kernel_initializer='normal',activation='relu'))\n",
    "model3.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "model3.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "#history = model3.fit(X_train, y_train, validation_split=0.05, epochs=500, batch_size=150, verbose=2)\n",
    "#model3.save_weights(\"NN3.h5\")\n",
    "\n",
    "model3.load_weights('NN3.h5')\n",
    "\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(2, input_dim=324, activation='relu'))\n",
    "model4.add(Dense(8, kernel_initializer='normal',activation='relu'))\n",
    "model4.add(Dense(8, kernel_initializer='normal',activation='relu'))\n",
    "model4.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n",
    "model4.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "#history = model4.fit(X_train, y_train, validation_split=0.05, epochs=500, batch_size=150, verbose=2)\n",
    "#model4.save_weights(\"NN4.h5\")\n",
    "\n",
    "#try different activation functions\n",
    "#different optimizers \n",
    "\n",
    "model4.load_weights('NN4.h5')\n",
    "\n",
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "params = {'n_estimators': 500, 'max_depth': 5, 'min_samples_split': 3, #3 samples fit, 250/10 vs 500/5\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "filename = 'GBoost1_m.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "params2 = {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 3, #3 samples fit, 250/10 vs 500/5\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf2 = ensemble.GradientBoostingRegressor(**params2)\n",
    "clf2.fit(X_train, y_train)\n",
    "filename = 'GBoost2_m.sav'\n",
    "pickle.dump(clf2, open(filename, 'wb'))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "gb1 = pickle.load(open('Gboost1_m.sav', 'rb'))\n",
    "gb2 = pickle.load(open('Gboost2_m.sav', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred5 = gb1.predict(X_test)\n",
    "y_pred5 = np.reshape(y_pred5,(1567,1))\n",
    "#rmse_gb = math.sqrt(mean_squared_error(y_test, y_pred5))\n",
    "#print(\"RMSE: %.4f\" % rmse_gb)\n",
    "\n",
    "\n",
    "y_pred6 = gb2.predict(X_test)\n",
    "y_pred6 = np.reshape(y_pred6,(1567,1))\n",
    "#rmse_gb = math.sqrt(mean_squared_error(y_test, y_pred6))\n",
    "#print(\"RMSE: %.4f\" % rmse_gb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "use this to calc rmse between two vectors of the same length\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def rmse_non_tensor(a,b):\n",
    "    total = 0\n",
    "    for i in range(len(a)):\n",
    "        total += (a[i]-b[i])**2\n",
    "    total = total/len(a)\n",
    "    total = math.sqrt(total)\n",
    "    return total\n",
    "\n",
    "y_pred1 = model1.predict(X_test) #model prediction on X test set\n",
    "y_pred2 = model2.predict(X_test) #model prediction on X test set\n",
    "y_pred3 = model3.predict(X_test) #model prediction on X test set\n",
    "y_pred4 = model4.predict(X_test) #model prediction on X test set\n",
    "\n",
    "\n",
    "#Ensemble of 4 ANNs + Gradient Boost Decision Tree Emsemble Regressor\n",
    "y_pred = ((0.1*(y_pred1 + y_pred2 + y_pred3 + y_pred4 + y_pred5) + 0.5*y_pred6))\n",
    "\n",
    "#y_pred = ((y_pred5 + y_pred6)/2.0)\n",
    "#y_test = np.reshape(y_test,(1567,1)) #no y_test\n",
    "#print (y_pred.shape)\n",
    "#print (y_test.shape)\n",
    "\n",
    "#print(math.sqrt(mean_squared_error(y_pred, y)))\n",
    "testing_labels = np.array(pd.read_csv('scoring.csv', delimiter = ',')['segment_id']) #testing data frame\n",
    "testing_labels = np.reshape(testing_labels, (1567,1))\n",
    "final = np.concatenate((testing_labels, y_pred), axis=1) #test is actual, pred is prediction\n",
    "final = pd.DataFrame(final)\n",
    "final.columns = ['segment_id', 'rate_of_penetration']\n",
    "#export final prediction csv\n",
    "export_csv = final.to_csv ('submission.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#summarize history for rmse\n",
    "plt.plot(history.history['rmse'])\n",
    "plt.plot(history.history['val_rmse'])\n",
    "plt.title('Model RMSE')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#cross val\n",
    "\n",
    "from sklearn.model_selection import KFold #K fold cross validation \n",
    "#X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "#y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "   \n",
    "   #lsvc = linear_model.Lasso(alpha=1, max_iter = 1000).fit(X_train, y_train)\n",
    "    #y_pred = lsvc.predict(X_test)\n",
    "    #regressor = RandomForestRegressor(n_estimators=100, max_depth = 15, random_state=0)\n",
    "    #regressor.fit(X_train, y_train)\n",
    "    #y_pred = regressor.predict(X_test)\n",
    "    #print (y_pred)\n",
    "    #print (y_test)\n",
    "    #print (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(2, input_dim=311, activation='linear'))\n",
    "    model.add(Dense(5, kernel_initializer='normal',activation='linear'))\n",
    "    model.add(Dense(5, kernel_initializer='normal',activation='linear'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    history = model.fit(X_train, y_train, epochs=500, batch_size=100, verbose=2)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
